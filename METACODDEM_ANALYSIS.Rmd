Analysis script for the MetaCoDDeM project. (plots + stats)
===========================================================
Author: Maxime Maheu [(C) Copyright 2014]

[All] Import data.
```{r [All] Importation, fig.width=7, fig.height=6}
rm(list = ls())
setwd("~/Dropbox (BEBG)/MetaCoDDeM project/Analysis/MetaCoDDeM analysis")

all_data <- read.csv("All_data.csv", header = TRUE, sep = ";")
attach(all_data)

questionnaire <- read.csv("Questionnaire.csv", header = TRUE, sep = ";")

subjects <- unique(all_data$Number)
N <- length(subjects)

AFC <- c(2, 24)
design <- AFC[1]
chancelevel <- 1/design

library(colorspace)
```

[All] Make the exclusions.
```{r [All] Exclusions, fig.width=7, fig.height=6}
deleted_data <- subset(all_data, all_data$Sig_Sig >= 0.3 | all_data$Number == 24)
deleted_subjects <- unique(deleted_data$Number)
all_data <- subset(all_data, all_data$Sig_Sig < 0.3 & all_data$Number != 24) # We delete all the subjects (4) with a theta bigger than 0.3, and an other subject because of its weired type II ROC curve (Aroc = 0.5).

for (deleted_subject in deleted_subjects) {
  questionnaire <- subset(questionnaire, questionnaire$Number != deleted_subject)
}

subjects <- vector()
for (subject in seq(N - length(deleted_subjects))) {
  subjects <- c(subjects, rep(subject, nrow(subset(all_data, all_data$Number == 1)))) }
all_data$Number <- subjects

subjects <- sort(unique(all_data$Number))
questionnaire$Number <- subjects
N <- length(subjects)
print(paste("N =", N))
```

[All] Make a summary.
```{r [All] Summary, fig.width=7, fig.height=6}
all_summary <- summary(all_data)
questionnaire_summary <- summary(questionnaire)

targeted_subject = 1
for (subject in seq(targeted_subject, targeted_subject)) {
  subject_table <- subset(all_data, all_data$Number == subject)
  subject_summary <- summary(subject_table) }
print(subject_summary)
```

[ALL] Plot the gain matrix.
```{r Points matrix, fig.width=4, fig.height=9}
P1_matrix <- matrix(c(50, -50), nrow = 1, ncol = 2, byrow = TRUE)
colnames(P1_matrix) <- c("Right", "Wrong")
rownames(P1_matrix) <- "Perceptual"
P2_matrix <- matrix(c(50, -110, 80, -80), nrow = 2, ncol = 2, byrow = TRUE)
colnames(P2_matrix) <- c("Right", "Wrong")
rownames(P2_matrix) <- c("Conf.", "Percep.")
P3_matrix <- matrix(c(50, -110, 130, -190, 110, -110, 90, -90, 70, -70, 50, -50), nrow = 6, ncol = 2, byrow = TRUE)
colnames(P3_matrix) <- c("Right", "Wrong")
rownames(P3_matrix) <- c("Conf.", "P. Now", "P. +0%", "P. +5%", "P. +10%", "P. +15%")

min_matrices <- min(c(P1_matrix, P2_matrix, P3_matrix))
max_matrices <- max(c(P1_matrix, P2_matrix, P3_matrix))

ColorRamp <- rev(diverge_hcl(abs(max_matrices) + abs(min_matrices)))
ColorLevels <- seq(min_matrices, max_matrices, length=length(ColorRamp))

plot.new()
layout(matrix(c(1,1,1,4, 2,2,2,4, 2,2,2,4, 3,3,3,4, 3,3,3,4, 3,3,3,4), nrow = 6, ncol = 4, byrow = TRUE), widths = c(1, 1), heights = c(1.25, 0.55, 1))

image(1:ncol(P1_matrix), 1:nrow(P1_matrix), t(P1_matrix), col = ColorRamp, axes = FALSE, zlim = c(min_matrices, max_matrices), xlab = "", ylab = "", main = "Phase 1")
a = 1; for (point in P2_matrix[1,]) {text(a, 1, point); a = a+1}
axis(side = 1, at = seq(1, length(colnames(P1_matrix)), 1), labels = colnames(P1_matrix))
axis(side = 2, at = seq(1, length(rownames(P1_matrix)), 1), labels = rownames(P1_matrix))
box(lwd = 2)

image(1:ncol(P2_matrix), 1:nrow(P2_matrix), t(P2_matrix), col = ColorRamp, axes = FALSE, zlim = c(min_matrices, max_matrices), xlab = "", ylab = "", main = "Phase 2")
a = 1; for (point in P2_matrix[,1]) {text(1, a, point); a = a+1}
a = 1; for (point in P2_matrix[,2]) {text(2, a, point); a = a+1}
axis(side = 1, at = seq(1, length(colnames(P2_matrix)), 1), labels = colnames(P2_matrix))
axis(side = 2, at = seq(1, length(rownames(P2_matrix)), 1), labels = rownames(P2_matrix))
box(lwd = 2)

image(1:ncol(P3_matrix), 1:nrow(P3_matrix), t(P3_matrix), col = ColorRamp, axes = FALSE, zlim = c(min_matrices, max_matrices), xlab = "", ylab = "", main = "Phase 3")
a = 1; for (point in P3_matrix[,1]) {text(1, a, point); a = a+1}
a = 1; for (point in P3_matrix[,2]) {text(2, a, point); a = a+1}
axis(side = 1, at = seq(1, length(colnames(P3_matrix)), 1), labels = colnames(P3_matrix))
axis(side = 2, at = seq(1, length(rownames(P3_matrix)), 1), labels = rownames(P3_matrix))
box(lwd = 2)

image(1, ColorLevels, matrix(data=ColorLevels, ncol=length(ColorLevels),nrow=1), col=ColorRamp,xlab="",ylab="",xaxt="n", las = 1)
box(lwd = 2)
```

[P1] Plot the psychometric curves and compute the intersect
```{r [P1] Sigmoids, fig.width=7, fig.height=6}
beta <- unique(all_data$Sig_Mu)
theta <- unique(all_data$Sig_Sig)

sigmoid <- function(x, beta, theta, chancelevel) {
  sig = chancelevel + ((1 - chancelevel) / (1 + exp(-beta*(x - theta))))
}

x = seq(0, 1, 0.001)
plot.new()
plot.window(xlim  = c(0, 1), ylim = c(0.5, 1), xaxs = "i", yaxs = "i")
grid(10, 5, lty = 2, lwd = 0.5)
for (subject in seq(1, N)) {
  lines(x, sigmoid(x, beta[subject], theta[subject], chancelevel), lwd = 1, col = "grey")
  }
lines(x, sigmoid(x, mean(beta), mean(theta), chancelevel), lwd = 5, col = "red")
axis(1, xaxp = c(0, 1, 10))
axis(2, xaxp = c(0.5, 1, 5))
title(xlab = "Motion coherence", ylab = "Accuracy", main = "Sigmoid curves fitted during P1")
legend("bottomright", c("Mean psychometric curve", "Individual psychometric curves"), col = c("red", "grey"), lwd = c(5, 1), box.lwd = 0, bg = "transparent")
box(lwd = 1)
```

[P1] Plot reaction times according to coherence. [Non terminÃ©]
```{r [P1] RT1 x coherence, fig.width=7, fig.height=6}
P1 <- subset(all_data, all_data$Phasis == 1)

cor.test(P1$RT1_brut, P1$A_coh)

plot.new()
plot.window(xlim  = c(0, 1.1), ylim = c(0, 7000), xaxs = "i", yaxs = "i")
grid(10, 5, lty = 2, lwd = 0.5)
points(P1$A_coh, P1$RT1_brut, pch = ".")
#abline(lm(P1$RT1_brut~sort(unique(P1$A_coh))), col = "red", lwd = 3)
axis(1, xaxp = c(0, 1, 10))
axis(2, xaxp = c(0, 7000, 14))

#text(80, 2000, paste("r =", as.character(round(cor.test(sort(unique(P2$Confidence)), RT_confidence)$estimate, digits = 2)), "\n", "p =", as.character(round(cor.test(sort(unique(P2$Confidence)), RT_confidence)$p.value, digits = 13))))
```

[P2] Do the SDT analysis and get the Aroc for each participant.
```{r [P2] ROC curves, fig.width=5, fig.height=5}
plot.new()
#layout(matrix(seq(1, N), 8, 4, byrow = TRUE))

mean_H_Aroc <- rep(0, 102)
mean_FA_Aroc <- rep(0, 102)
individual_H_Aroc <- matrix(NA, nrow = N, ncol = length(seq(0, 100)) + 1)
individual_FA_Aroc <- matrix(NA, nrow = N, ncol = length(seq(0, 100)) + 1)

d_prime <- vector()
Aroc <- vector()

for (subject in subjects) { #subjects seq(1,1)
  subject_table <- subset(all_data, all_data$Number == subject & all_data$Phasis == 2)
  table <- table(factor(subject_table$Label, levels = 1:4), factor(subject_table$Confidence, levels = 0:100))
  typeI_table <- table + 0 #0.5
  
  p_hit = vector()
  p_false_alarm = vector()
  p_hit[1] = 0
  p_false_alarm[1] = 0
  
  i = 2
  for (confidence in seq(100, 0, by = -1)) {
    p_hit[i] = (typeI_table[1, confidence + 1]/(sum(typeI_table[1,]) + sum(typeI_table[3,]))) + p_hit[i - 1]
    p_false_alarm[i] = (typeI_table[2, confidence + 1]/(sum(typeI_table[2,]) + sum(typeI_table[4,]))) + p_false_alarm[i - 1]
    i = i + 1}
  rm(confidence)
  
  i = 100 + 3
  for (confidence in seq(0, 100)) {
    p_hit[i] = (typeI_table[3, confidence + 1]/(sum(typeI_table[1,]) + sum(typeI_table[3,]))) + p_hit[i - 1]
    p_false_alarm[i] = (typeI_table[4, confidence + 1]/(sum(typeI_table[2,]) + sum(typeI_table[4,]))) + p_false_alarm[i - 1]
    i = i + 1}
  rm(confidence)
  
  p_hits = sum(typeI_table[1,])/(sum(typeI_table[1,]) + sum(typeI_table[3,]))
  p_false_alarms = sum(typeI_table[2,])/(sum(typeI_table[2,]) + sum(typeI_table[4,]))
  d_prime[subject] = (qnorm(p_hits, 0, 1) - qnorm(p_false_alarms, 0, 1))*(1/sqrt(2))
  
  hit = table[1, (0 + 1):(100 + 1)]
  false_alarm = table[2, (0 + 1):(100 + 1)]
  miss = table[3, (0 + 1):(100 + 1)]
  correct_reject = table[4, (0 + 1):(100 + 1)]
  
  H = hit + correct_reject + 0.5
  FA = false_alarm + miss + 0.5
  
  pH <- vector()
  pFA <- vector()
  for (confidence in seq(0, 100)) {
    pH[confidence + 1] = H[confidence + 1] / sum(H)
    pFA[confidence + 1] = FA[confidence + 1] / sum(FA)}
  rm(confidence)
  
  pH_cum <- vector()
  pFA_cum <- vector()
  pH_cum[1] = 0
  pFA_cum[1] = 0
  for (confidence in seq((0 + 2), (100 + 2))) {
    pH_cum[confidence] = pH[(100 + 3) - confidence] + pH_cum[confidence - 1]
    pFA_cum[confidence] = pFA[(100 + 3) - confidence] + pFA_cum[confidence - 1]}
  rm(confidence)
  
  mean_H_Aroc <- mean_H_Aroc + pH_cum
  mean_FA_Aroc <- mean_FA_Aroc + pFA_cum
  individual_H_Aroc[subject,] <- pH_cum
  individual_FA_Aroc[subject,] <- pFA_cum
  
  ka <- vector()
  kb <- vector()
  for (confidence in seq((0 + 2), 50)) {
    ka[confidence - 1] = (pH_cum[confidence] - pFA_cum[confidence - 1])^2 - (pH_cum[confidence - 1] - pFA_cum[confidence])^2}
  rm(confidence)
  for (confidence in seq(50, (100 + 1))) {
    kb[confidence - 50] = (pH_cum[confidence] - pFA_cum[confidence - 1])^2 - (pH_cum[confidence - 1] - pFA_cum[confidence])^2}
  rm(confidence)
  ka = sum(ka)
  kb = sum(kb)

  Aroc[subject] = (1/2) + ((1/4)*ka) + ((1/4)*kb)
  
  plot.new()
  plot.window(xlim  = c(0, 1), ylim = c(0, 1), xaxs = "i", yaxs = "i")
  grid(10, 10, lty = 2, lwd = 0.5)
  
  #lines(x = p_false_alarm, y = p_hit, lty = 2)
  polygon(x = p_false_alarm, y = p_hit, border = "gray87", col = "gray87")
  #lines(x = pFA_cum, y = pH_cum, col = "red")
  polygon(x = pFA_cum, y = pH_cum, border = "lightskyblue2", col = "lightskyblue2")
  lines(x = c(0,1), y = c (0,1))
  
  text(x = 0.1, y = 0.9, substitute(A["ROC"] == this, list(this = round(Aroc[subject], 2))) , adj = 0)
  axis(1, xaxp = c(0, 1, 5))
  axis(2, yaxp = c(0, 1, 5))
  title(xlab = "P (confidence | incorrect) or FA rate", ylab = "P (confidence | correct) or H rate", main = paste("ROC curves for subject", subject))  
  legend("bottomright", c("Type I", "Type II", "Chance"), lty = c(0, 0, 1), pch = c(15, 15, NA), col = c("gray87", "lightskyblue2", "black"), box.lwd = 0, bg = "transparent")
  box(lwd = 1)
}
```

[P2] ROC curves again.
```{r [P2] All ROC curves, fig.width=8, fig.height=8}
plot.new()
plot.window(xlim  = c(0, 1), ylim = c(0, 1), xaxs = "i", yaxs = "i")
grid(10, 10, lty = 2, lwd = 0.5)

for (subject in subjects) {
  lines(x = individual_FA_Aroc[subject,], y = individual_H_Aroc[subject,], col = "grey", lwd = 1) }
lines(x = c(0,1), y = c (0,1))
polygon(x = (mean_FA_Aroc/N), y = (mean_H_Aroc/N), col = rgb(8/255, 95/255, 205/255, 0.5))
lines(x = (mean_FA_Aroc/N), y = (mean_H_Aroc/N), col = "royalblue3", lwd = 5)

axis(1, xaxp = c(0, 1, 10))
axis(2, yaxp = c(0, 1, 10))
title(xlab = "P (confidence | incorrect)", ylab = "P (confidence | correct)", main = "ROC curves coming from P2")
legend("bottomright", c("Mean type II ROC curve", "Individual type II ROC curves"), col = c("royalblue3", "grey"), lwd = c(5, 1), box.lwd = 0, bg = "transparent")
box(lwd = 1)
```

[P2] Plot RT against confidence. [Non terminÃ©]
```{r fig.width=7, fig.height=6}
P2 <- subset(all_data, all_data$Phasis == 2)
P2 <- subset(P2, P2$RT1_brut < 4000)
RT_confidence <- tapply(P2$RT1_brut, P2$Confidence, mean)

#matrix <- matrix(, nrow = 2,, ncol = length(g))
#RT_confidence <- 

cor.test(sort(unique(P2$Confidence)), RT_confidence)

plot(sort(unique(P2$Confidence)), RT_confidence, pch = 20, frame.plot = FALSE, xlab = "Confidence ratings", ylab = "Perceptual reaction time", main = "Correlation between confidence and perceptual RT during P2", xaxs = "i", yaxs = "i", ylim = c(800, 2500))
abline(lm(RT_confidence~sort(unique(P2$Confidence))), col = "red", lwd = 3)
text(80, 2000, paste("r =", as.character(round(cor.test(sort(unique(P2$Confidence)), RT_confidence)$estimate, digits = 2)), "\n", "p =", as.character(round(cor.test(sort(unique(P2$Confidence)), RT_confidence)$p.value, digits = 13))))
```

[P2] Plot performance against level of information (None, +0, +5, +10, +15) for the 3 basal performance ().
```{r fig.width=7, fig.height=6}
# Faire Ã§a uniquement pour P2 (pour l'instant) afin de montrer que le 2e Ã©chantillon apporte de l'information

P2 <- subset(all_data, all_data$Phasis == 2)
performance_IS <- table(P2$Accuracy, P2$Inc_perf, P2$A_perf)#/nrow(P2)

plot.new()
plot.window(xlim  = c(1, 5), ylim = c(0.5, 1), xaxs = "i", yaxs = "i")
grid(4, 5, lty = 2, lwd = 0.5)

for (basal_performance in seq(1, length(unique(P2$A_perf)))) {
  for (increasing_performance in seq(1, 5)) {
    performance_IS[,increasing_performance,basal_performance] <- performance_IS[,increasing_performance,basal_performance]/sum(performance_IS[,increasing_performance,basal_performance])
  }
  
  lines(performance_IS[2,,basal_performance], col = "blue")
  lines(x = seq(1, 5), y = c(sort(unique(P2$A_perf))[basal_performance], sort(unique(P2$A_perf))[basal_performance] + 0, sort(unique(P2$A_perf))[basal_performance] + 0.05, sort(unique(P2$A_perf))[basal_performance] + 0.10, sort(unique(P2$A_perf))[basal_performance] +  0.15), col = "red")
}

axis(1, at = seq(1, 5), labels = rownames(gain_matrix))
axis(2)

# Maintenant faire Ã§a en soustrayant le niveau basal de performance

plot.new()
plot.window(xlim  = c(1, 5), ylim = c(0, 0.25), xaxs = "i", yaxs = "i")
grid(4, 5, lty = 2, lwd = 0.5)

for (basal_performance in seq(1, 3)) {
  for (increasing_performance in seq(1, 5)) {
    performance_IS[,increasing_performance,basal_performance] <- performance_IS[,increasing_performance,basal_performance]/sum(performance_IS[,increasing_performance,basal_performance])
  }
  
  lines(performance_IS[2,,basal_performance] - sort(unique(P2$A_perf))[basal_performance], col = "blue")
  lines(x = seq(1, 5), y = c(0, 0, .05, .10, .15), col = "red")
}

axis(1, at = seq(1, 5), labels = rownames(gain_matrix))
axis(2)
```

[P2] Fit the LBA model.
#```{r fig.width=7, fig.height=6}
source("pq-lba.r")
qps = seq(0.1, 0.9, 0.2)
trim = c(180, 10000)

rawdata = subset(all_data, all_data$Phasis == 2, select = c(Condition, Accuracy, RT1_brut))
colnames(rawdata) <- c("difficulty", "correct", "rt")

use = (rawdata$rt > trim[1]) & (rawdata$rt < trim[2])
nms = list(c("err","crct"), c("1", "2", "3", "4", "5", "6", "7", "8", "9", "10", "11", "12", "13", "14", "15"))
dims = unlist(lapply(nms, length))

q = tapply(rawdata$rt[use], list(rawdata$correct[use], rawdata$difficulty[use]), quantile, probs = qps)
q = array(unlist(q), dim = c(length(qps), dims), dimnames = c(list(qps), nms))
n = tapply(rawdata$rt[use], list(rawdata$correct[use], rawdata$difficulty[use]), length)
p = tapply(rawdata$correct[use], list(rawdata$difficulty[use]), mean)

dimnames(n) = nms ; dimnames(p) = nms[-1]
pb = array(rep(n, each = length(qps) + 1), dim = c(length(qps) + 1, dim(n)), dimnames = c(list(NULL) ,dimnames(n)))*c(.1,.2,.2,.2,.2,.1)
data = list(p = p, n = n, q = q, pb = pb)
ndrifts = 15

fit = fitter(dat = data, maxit = c(100, 200, 500, 1000, 2000, 5000), nc = ndrifts)
pars = c(exp(fit[1:4]), fit[5:19])

pars[4] = pars[4] + pars[2]
pars[5:7] = 1 - pars[5:7]
fit[5:7] = 1 - fit[5:7]

names(pars) = c("s","A","ter","b","0.65","0.75","0.85","0.65+0.00","0.75+0.00","0.85+0.00","0.65+0.05","0.75+0.05","0.85+0.05","0.65+0.10","0.75+0.10","0.85+0.10","0.65+0.15","0.75+0.15","0.85+0.15")
print(pars, 3)

plot.new()
plot.window(xlim  = c(0, 2000), ylim = c(0, 1200), xaxs = "i", yaxs = "i")
grid(10, 5, lty = 2, lwd = 0.5)

for (drift_rate in seq(5, 19)) {
  abline(a = pars[2], b = pars[drift_rate], col = "green")
}

abline(h = pars[4], col = "red")

```

[P3] Create the OPIS (Optimal Proactive Information Seeking) model.
```{r [P3] OPIS model, fig.width=7, fig.height=6}
# Se restreindre aux niveaux de performance initiaux >= .70

#CrÃ©Ã©r une matrice de gain en fonction des rÃ©ponses (correctes et erreurs) et du niveau d'information demandÃ©.
if (design == AFC[1]) {
  gain_matrix <- matrix(c(130, -190, 110, -110, 90, -90, 70, -70, 50, -50), nrow = 5, ncol = 2, byrow = TRUE)
}
if (design == AFC[2]) {
  gain_matrix <- matrix(c(200, -200, 160, -110, 140, -90, 100, -70, 60, -50), nrow = 5, ncol = 2, byrow = TRUE)
}
colnames(gain_matrix) <- c("Right answer", "Wrong answer")
rownames(gain_matrix) <- c("No", "+0%", "+5%", "+10%", "+15%")
facility_levels = c(0, 0.05, 0.1, 0.15) # Without immediate answer
performance_list <- seq(chancelevel, 1 - max(facility_levels) - 0.01, (1 - max(facility_levels) - chancelevel - 0.01)/15)
print(gain_matrix)

# Trouver le nombre de point moyen gagnÃ© par trial en fonction de la probabilitÃ© d'avoir une rÃ©ponse correcte (performance finale) et le nombre de points (positif et nÃ©gatif) qu'engendre chaque niveau d'information supplÃ©mentaire.
model <- matrix(rep(NA, length(performance_list)*length(gain_matrix[,1])), nrow = length(performance_list), ncol = length(gain_matrix[,1]))
temp = c(0, facility_levels)
for (p in seq(1:length(performance_list))) {
  for (f in seq(1:length(gain_matrix[,1]))) {
    model[p,f] <- (gain_matrix[f,1] * (performance_list[p] + temp[f])) + (gain_matrix[f,2] * (1 - (performance_list[p] + temp[f])))
  }
}
colnames(model) <- c("No", "+0%", "+5%", "+10%", "+15%")
rownames(model) <- round(performance_list, 2)
print(model)

# Trouver, parmi cette matrice, le niveau d'information supplÃ©mentaire assurant la plus grand nombre de point (croisement entre performance finale et gain potentiel).
optim <- rep(NA, length(performance_list))
values <- rep(NA, length(performance_list))
for (i in seq(1, length(performance_list), 1)) {
  value <- max(which(model[i,] == max(model[i,]), arr.ind = TRUE))
  values[i] <- value
  optim[i] <- colnames(model)[value]
}
print(optim)

# Plotter l'Ã©volution du nombre de points thÃ©oriquement obtenus pour chaque niveau d'information supplÃ©mentaire dans chaque situation de performance initiale.
heatcol <- rev(heat.colors(length(colnames(model))))
barplot(t(model), beside = TRUE, ylim = c(min(model)-10, max(model) + 10), col = heatcol, xlab = "Initial performance", ylab = "Mean points")
for (i in seq(1, length(performance_list), 1)) {
  text(x = (performance_list[i]*270) - 135, y = max(model) + 5, optim[i]) 
}
legend(x = length(colnames(model))*length(rownames(model)), y = -(abs(0-min(model)))/3, colnames(model), box.lwd = 0, pch = 15, col = (heatcol))

# Plotter l'Ã©volution optimale du switch entre les niveaux d'information supplÃ©mentaire Ã  demandÃ© dans le but d'avoir le maximum de points possible.
seek <- rep(NA, length(values))
info = c(-0.05, facility_levels)
for (i in seq(1, length(values), 1)) {
  seek[i] = info[values[i]]
}
plot(x = performance_list, y = seek, type = "b", pch = 15, lwd = 2, col = "orange", frame.plot = FALSE, xlim = c(chancelevel, 1 - max(facility_levels)), xlab = "Initial performance", ylab = "Proactive information seeking coefficient", main = "Optimal Proactive Information Seeking model")

for (level in seq(1, length(seek))) {
  if (seek[level] == 0.15) {seek[level] = 5}
  if (seek[level] == 0.10) {seek[level] = 4}
  if (seek[level] == 0.05) {seek[level] = 3}
  if (seek[level] == 0) {seek[level] = 2}
  if (seek[level] == -0.05) {seek[level] = 1}
}
```

[P3] 
```{r delta OPIS, fig.width=7, fig.height=6}
heatcol <- rev(heat.colors(nrow(IS)))
heatcol[4] <- "darkorange2"

plot.new()
plot.window(xlim  = c(0.5, 0.84), ylim = c(1, 5), xaxs = "i", yaxs = "i")
grid(length(seek), 6, lty = 2, lwd = 0.5)

subject_IS <- matrix(, nrow = N, ncol = length(seek))
mean_IS <- rep(0, length(seek))
delta_OPIS <- rep(0, length(seek))

for (subject in subjects) {
  subject_table <- subset(all_data, all_data$Number == subject & all_data$Phasis == 3)
  IS_table <- table(subject_table$A_perf, factor(subject_table$Inc_perf, levels = c(-1, 0, 0.05, 0.10, 0.15)))
  IS_table <- IS_table/sum(IS_table[1,])
  colnames(IS_table) <- c("No", "+0%", "+5%", "+10%", "+15%")
  
  for (basal_performance in seq(1, nrow(IS_table))) {
    # Version avec le niveau moyen de recherche d'information
    subject_IS[subject, basal_performance] <- IS_table[basal_performance,1]*1 + IS_table[basal_performance,2]*2 + IS_table[basal_performance,3]*3 + IS_table[basal_performance,4]*4 + IS_table[basal_performance,5]*5
    
    # Version avec le niveau de recherche d'information le plus souvent choisit
    # subject_IS[subject, basal_performance] <- min(which(IS_table[basal_performance,] == max(IS_table[basal_performance,1], IS_table[basal_performance,2], IS_table[basal_performance,3], IS_table[basal_performance,4], IS_table[basal_performance,5])))
  }
  
  lines(x = as.numeric(rownames(IS_table)), y = subject_IS[subject,], col = "grey")
  
  mean_IS <- mean_IS + subject_IS[subject,]
  delta_OPIS[subject] <- sum(abs(subject_IS[subject,]-seek))/length(seek)
  
  #seek[1:7]<-NA
  #delta_OPIS[subject] <- sum((na.omit(subject_IS[subject,])-na.omit(seek))^2)/length(na.omit(seek))
}

lines(x = as.numeric(rownames(IS_table)), y = mean_IS/N, col = "green", lwd = 5)
lines(x = as.numeric(rownames(IS_table)), y = seek, col = "red", lwd = 5)

axis(1, at = as.numeric(rownames(IS_table)))
axis(2, at = c(1,2,3,4,5), labels = rownames(gain_matrix))

title(xlab = "Basal performance", ylab = "Information seeking level", main = "Information seeking behavior during P3")
box(lwd = 1)
```

[P3] Plot the amount of information seeking for each basal performance (and the optimal level).
```{r fig.width=10, fig.height=10}
P3 <- subset(all_data, all_data$Phasis == 3)
IS <- table(P3$Inc_perf, P3$A_perf)
for (basal_performance in seq(1, ncol(IS))) {
  IS[,basal_performance] = IS[,basal_performance]/sum(IS[,basal_performance])
}

plot.new()
plot.window(xlim  = c(0.5, 1), ylim = c(0, 1), xaxs = "i", yaxs = "i")
heatcol <- rev(heat_hcl(nrow(IS))) #heat.colors #heatcol[4] = "darkorange2"

barplot(IS, col = heatcol, space = 0, border = NA, xaxs = "i", yaxs = "i")
abline(v = 7, lty = 2)

axis(1)
axis(2, yaxp = c(0, 1, 10))
title(xlab = "Sigmoidal evoked basal performance (first sample)", ylab = "Frequency of information seeking levels", main = "Dynamics of information seeking behavior according to initial difficulty during P3")
legend("bottomright", legend = c("+15%", "+10%", "+5%", "+0%", "No"), rownames(IS), pch = 15, col = rev(heatcol), bg = "white")
box(lwd = 1)
```

[P3] Plot confidence ratings against information seeking level.
```{r [P3] Confidence x IS, fig.width=7, fig.height=6}
CIS <- table(P3$Inc_perf, P3$Confidence)#factor(P3$Confidence, levels = 0:100))

for (confidence in seq(1, length(colnames(CIS)))) {
  CIS[,confidence] <- CIS[,confidence]/sum(CIS[,confidence])
}

heatcol <- rev(heat_hcl(nrow(IS)))
barplot(CIS, col = heatcol, space = 0, border = NA)

title(xlab = "Sigmoidal evoked basal performance (first sample)", ylab = "Frequency of information seeking levels", main = "Dynamics of information seeking behavior according to initial difficulty")
legend("bottomright", legend = c("+15%", "+10%", "+5%", "+0%", "No"), rownames(IS), pch = 15, col = rev(heatcol))
```

[P3] RT1 x ISL
```{r}
plot(P3$Inc_perf, P3$RT1_brut)
oneway.test(P3$RT1_brut ~ as.character(P3$Inc_perf), P3)
tapply(P3$RT1_brut, P3$Inc_perf, mean)
barplot(tapply(P3$RT1_brut, P3$Inc_perf, mean))
a = aov(lm(P3$RT1_brut ~ as.character(P3$Inc_perf)))
TukeyHSD(a)
```


[P3] Use the LBA parameters and correlate them with the information seeking dynamic.
```{r fig.width=7, fig.height=6}
#LBA_inbut <- subset(all_data, all_data$Phasis == 3, select = c(all_data$RT1_brut, all_data$Condition, all_data$Accuracy))
```