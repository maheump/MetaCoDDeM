Analysis script for the MetaCoDDeM project. (plots + stats)
===========================================================
Author: Maxime Maheu [(C) Copyright 2014]

[All] Import data.
```{r [All] Importation, fig.width=7, fig.height=6}
rm(list = ls())
setwd("~/Dropbox (BEBG)/MetaCoDDeM project/Analysis/MetaCoDDeM analysis")

all_data <- read.csv("All_data.csv", header = TRUE, sep = ";")
attach(all_data)

questionnaire <- read.csv("Questionnaire.csv", header = TRUE, sep = ";")

subjects <- unique(all_data$Number)
N <- length(subjects)

AFC <- c(2, 24)
design <- AFC[1]
chancelevel <- 1/design

library(colorspace)
```

[All] Make the exclusions.
```{r [All] Exclusions, fig.width=7, fig.height=6}
deleted_data <- subset(all_data, all_data$Sig_Sig >= 0.3 | all_data$Number == 24)
deleted_subjects <- unique(deleted_data$Number)
all_data <- subset(all_data, all_data$Sig_Sig < 0.3 & all_data$Number != 24) # We delete all the subjects (4) with a theta bigger than 0.3, and an other subject because of its weired type II ROC curve (Aroc = 0.5).

for (deleted_subject in deleted_subjects) {
  questionnaire <- subset(questionnaire, questionnaire$Number != deleted_subject)
}

subjects <- vector()
for (subject in seq(N - length(deleted_subjects))) {
  subjects <- c(subjects, rep(subject, nrow(subset(all_data, all_data$Number == 1)))) }
all_data$Number <- subjects

subjects <- sort(unique(all_data$Number))
questionnaire$Number <- subjects
N <- length(subjects)
print(paste("N =", N))
```

[All] Make a summary.
```{r [All] Summary, fig.width=7, fig.height=6}
all_summary <- summary(all_data)
questionnaire_summary <- summary(questionnaire)

targeted_subject = 1
for (subject in seq(targeted_subject, targeted_subject)) {
  subject_table <- subset(all_data, all_data$Number == subject)
  subject_summary <- summary(subject_table) }
print(subject_summary)
```

[ALL] Plot the gain matrix.
```{r Points matrix, fig.width=4, fig.height=9}
P1_matrix <- matrix(c(50, -50), nrow = 1, ncol = 2, byrow = TRUE)
colnames(P1_matrix) <- c("Right", "Wrong")
rownames(P1_matrix) <- "Perceptual"
P2_matrix <- matrix(c(50, -110, 80, -80), nrow = 2, ncol = 2, byrow = TRUE)
colnames(P2_matrix) <- c("Right", "Wrong")
rownames(P2_matrix) <- c("Conf.", "Percep.")
P3_matrix <- matrix(c(50, -110, 130, -190, 110, -110, 90, -90, 70, -70, 50, -50), nrow = 6, ncol = 2, byrow = TRUE)
colnames(P3_matrix) <- c("Right", "Wrong")
rownames(P3_matrix) <- c("Conf.", "P. Now", "P. +0%", "P. +5%", "P. +10%", "P. +15%")

min_matrices <- min(c(P1_matrix, P2_matrix, P3_matrix))
max_matrices <- max(c(P1_matrix, P2_matrix, P3_matrix))

ColorRamp <- rev(diverge_hcl(abs(max_matrices) + abs(min_matrices)))
ColorLevels <- seq(min_matrices, max_matrices, length=length(ColorRamp))

plot.new()
layout(matrix(c(1,1,1,4, 2,2,2,4, 2,2,2,4, 3,3,3,4, 3,3,3,4, 3,3,3,4), nrow = 6, ncol = 4, byrow = TRUE), widths = c(1, 1), heights = c(1.25, 0.55, 1))

image(1:ncol(P1_matrix), 1:nrow(P1_matrix), t(P1_matrix), col = ColorRamp, axes = FALSE, zlim = c(min_matrices, max_matrices), xlab = "", ylab = "", main = "Phase 1")
a = 1; for (point in P2_matrix[1,]) {text(a, 1, point); a = a+1}
axis(side = 1, at = seq(1, length(colnames(P1_matrix)), 1), labels = colnames(P1_matrix))
axis(side = 2, at = seq(1, length(rownames(P1_matrix)), 1), labels = rownames(P1_matrix))
box(lwd = 2)

image(1:ncol(P2_matrix), 1:nrow(P2_matrix), t(P2_matrix), col = ColorRamp, axes = FALSE, zlim = c(min_matrices, max_matrices), xlab = "", ylab = "", main = "Phase 2")
a = 1; for (point in P2_matrix[,1]) {text(1, a, point); a = a+1}
a = 1; for (point in P2_matrix[,2]) {text(2, a, point); a = a+1}
axis(side = 1, at = seq(1, length(colnames(P2_matrix)), 1), labels = colnames(P2_matrix))
axis(side = 2, at = seq(1, length(rownames(P2_matrix)), 1), labels = rownames(P2_matrix))
box(lwd = 2)

image(1:ncol(P3_matrix), 1:nrow(P3_matrix), t(P3_matrix), col = ColorRamp, axes = FALSE, zlim = c(min_matrices, max_matrices), xlab = "", ylab = "", main = "Phase 3")
a = 1; for (point in P3_matrix[,1]) {text(1, a, point); a = a+1}
a = 1; for (point in P3_matrix[,2]) {text(2, a, point); a = a+1}
axis(side = 1, at = seq(1, length(colnames(P3_matrix)), 1), labels = colnames(P3_matrix))
axis(side = 2, at = seq(1, length(rownames(P3_matrix)), 1), labels = rownames(P3_matrix))
box(lwd = 2)

image(1, ColorLevels, matrix(data=ColorLevels, ncol=length(ColorLevels),nrow=1), col=ColorRamp,xlab="",ylab="",xaxt="n", las = 1)
box(lwd = 2)
```

[P1] Plot the psychometric curves and compute the intersect
```{r [P1] Sigmoids, fig.width=7, fig.height=6}
beta <- unique(all_data$Sig_Mu)
theta <- unique(all_data$Sig_Sig)

sigmoid <- function(x, beta, theta, chancelevel) {
  sig = chancelevel + ((1 - chancelevel) / (1 + exp(-beta*(x - theta))))
}

x = seq(0, 1, 0.001)
plot.new()
plot.window(xlim  = c(0, 1), ylim = c(0.5, 1), xaxs = "i", yaxs = "i")
grid(10, 5, lty = 2, lwd = 0.5)
for (subject in seq(1, N)) {
  lines(x, sigmoid(x, beta[subject], theta[subject], chancelevel), lwd = 1, col = "grey")
  }
lines(x, sigmoid(x, mean(beta), mean(theta), chancelevel), lwd = 5, col = "red")
axis(1, xaxp = c(0, 1, 10))
axis(2, xaxp = c(0.5, 1, 5))
title(xlab = "Motion coherence", ylab = "Accuracy", main = "Sigmoid curves fitted during P1")
legend("bottomright", c("Mean psychometric curve", "Individual psychometric curves"), col = c("red", "grey"), lwd = c(5, 1), box.lwd = 0, bg = "transparent")
box(lwd = 1)
```

[P1] Plot reaction times according to coherence. [Non terminé]
```{r [P1] RT1 x coherence, fig.width=7, fig.height=6}
P1 <- subset(all_data, all_data$Phasis == 1)

cor.test(P1$RT1_brut, P1$A_coh)

plot.new()
plot.window(xlim  = c(0, 1.1), ylim = c(0, 7000), xaxs = "i", yaxs = "i")
grid(10, 5, lty = 2, lwd = 0.5)
points(P1$A_coh, P1$RT1_brut, pch = ".")
#abline(lm(P1$RT1_brut~sort(unique(P1$A_coh))), col = "red", lwd = 3)
axis(1, xaxp = c(0, 1, 10))
axis(2, xaxp = c(0, 7000, 14))

#text(80, 2000, paste("r =", as.character(round(cor.test(sort(unique(P2$Confidence)), RT_confidence)$estimate, digits = 2)), "\n", "p =", as.character(round(cor.test(sort(unique(P2$Confidence)), RT_confidence)$p.value, digits = 13))))
```

[P2] Do the SDT analysis and get the Aroc for each participant.
```{r [P2] ROC curves, fig.width=5, fig.height=5}
plot.new()
#layout(matrix(seq(1, N), 8, 4, byrow = TRUE))

mean_H_Aroc <- rep(0, 102)
mean_FA_Aroc <- rep(0, 102)
individual_H_Aroc <- matrix(NA, nrow = N, ncol = length(seq(0, 100)) + 1)
individual_FA_Aroc <- matrix(NA, nrow = N, ncol = length(seq(0, 100)) + 1)

d_prime <- vector()
Aroc <- vector()

for (subject in subjects) { #subjects seq(1,1)
  subject_table <- subset(all_data, all_data$Number == subject & all_data$Phasis == 2)
  table <- table(factor(subject_table$Label, levels = 1:4), factor(subject_table$Confidence, levels = 0:100))
  typeI_table <- table + 0 #0.5
  
  p_hit = vector()
  p_false_alarm = vector()
  p_hit[1] = 0
  p_false_alarm[1] = 0
  
  i = 2
  for (confidence in seq(100, 0, by = -1)) {
    p_hit[i] = (typeI_table[1, confidence + 1]/(sum(typeI_table[1,]) + sum(typeI_table[3,]))) + p_hit[i - 1]
    p_false_alarm[i] = (typeI_table[2, confidence + 1]/(sum(typeI_table[2,]) + sum(typeI_table[4,]))) + p_false_alarm[i - 1]
    i = i + 1}
  rm(confidence)
  
  i = 100 + 3
  for (confidence in seq(0, 100)) {
    p_hit[i] = (typeI_table[3, confidence + 1]/(sum(typeI_table[1,]) + sum(typeI_table[3,]))) + p_hit[i - 1]
    p_false_alarm[i] = (typeI_table[4, confidence + 1]/(sum(typeI_table[2,]) + sum(typeI_table[4,]))) + p_false_alarm[i - 1]
    i = i + 1}
  rm(confidence)
  
  p_hits = sum(typeI_table[1,])/(sum(typeI_table[1,]) + sum(typeI_table[3,]))
  p_false_alarms = sum(typeI_table[2,])/(sum(typeI_table[2,]) + sum(typeI_table[4,]))
  d_prime[subject] = (qnorm(p_hits, 0, 1) - qnorm(p_false_alarms, 0, 1))*(1/sqrt(2))
  
  hit = table[1, (0 + 1):(100 + 1)]
  false_alarm = table[2, (0 + 1):(100 + 1)]
  miss = table[3, (0 + 1):(100 + 1)]
  correct_reject = table[4, (0 + 1):(100 + 1)]
  
  H = hit + correct_reject + 0.5
  FA = false_alarm + miss + 0.5
  
  pH <- vector()
  pFA <- vector()
  for (confidence in seq(0, 100)) {
    pH[confidence + 1] = H[confidence + 1] / sum(H)
    pFA[confidence + 1] = FA[confidence + 1] / sum(FA)}
  rm(confidence)
  
  pH_cum <- vector()
  pFA_cum <- vector()
  pH_cum[1] = 0
  pFA_cum[1] = 0
  for (confidence in seq((0 + 2), (100 + 2))) {
    pH_cum[confidence] = pH[(100 + 3) - confidence] + pH_cum[confidence - 1]
    pFA_cum[confidence] = pFA[(100 + 3) - confidence] + pFA_cum[confidence - 1]}
  rm(confidence)
  
  mean_H_Aroc <- mean_H_Aroc + pH_cum
  mean_FA_Aroc <- mean_FA_Aroc + pFA_cum
  individual_H_Aroc[subject,] <- pH_cum
  individual_FA_Aroc[subject,] <- pFA_cum
  
  ka <- vector()
  kb <- vector()
  for (confidence in seq((0 + 2), 50)) {
    ka[confidence - 1] = (pH_cum[confidence] - pFA_cum[confidence - 1])^2 - (pH_cum[confidence - 1] - pFA_cum[confidence])^2}
  rm(confidence)
  for (confidence in seq(50, (100 + 1))) {
    kb[confidence - 50] = (pH_cum[confidence] - pFA_cum[confidence - 1])^2 - (pH_cum[confidence - 1] - pFA_cum[confidence])^2}
  rm(confidence)
  ka = sum(ka)
  kb = sum(kb)

  Aroc[subject] = (1/2) + ((1/4)*ka) + ((1/4)*kb)
  
  plot.new()
  plot.window(xlim  = c(0, 1), ylim = c(0, 1), xaxs = "i", yaxs = "i")
  grid(10, 10, lty = 2, lwd = 0.5)
  
  #lines(x = p_false_alarm, y = p_hit, lty = 2)
  polygon(x = p_false_alarm, y = p_hit, border = "gray87", col = "gray87")
  #lines(x = pFA_cum, y = pH_cum, col = "red")
  polygon(x = pFA_cum, y = pH_cum, border = "lightskyblue2", col = "lightskyblue2")
  lines(x = c(0,1), y = c (0,1))
  
  text(x = 0.1, y = 0.9, substitute(A["ROC"] == this, list(this = round(Aroc[subject], 2))) , adj = 0)
  axis(1, xaxp = c(0, 1, 5))
  axis(2, yaxp = c(0, 1, 5))
  title(xlab = "P (confidence | incorrect) or FA rate", ylab = "P (confidence | correct) or H rate", main = paste("ROC curves for subject", subject))  
  legend("bottomright", c("Type I", "Type II", "Chance"), lty = c(0, 0, 1), pch = c(15, 15, NA), col = c("gray87", "lightskyblue2", "black"), box.lwd = 0, bg = "transparent")
  box(lwd = 1)
}
```

[P2] ROC curves again.
```{r [P2] All ROC curves, fig.width=8, fig.height=8}
plot.new()
plot.window(xlim  = c(0, 1), ylim = c(0, 1), xaxs = "i", yaxs = "i")
grid(10, 10, lty = 2, lwd = 0.5)

for (subject in subjects) {
  lines(x = individual_FA_Aroc[subject,], y = individual_H_Aroc[subject,], col = "grey", lwd = 1) }
lines(x = c(0,1), y = c (0,1))
polygon(x = (mean_FA_Aroc/N), y = (mean_H_Aroc/N), col = rgb(8/255, 95/255, 205/255, 0.5))
lines(x = (mean_FA_Aroc/N), y = (mean_H_Aroc/N), col = "royalblue3", lwd = 5)

axis(1, xaxp = c(0, 1, 10))
axis(2, yaxp = c(0, 1, 10))
title(xlab = "P (confidence | incorrect)", ylab = "P (confidence | correct)", main = "ROC curves coming from P2")
legend("bottomright", c("Mean type II ROC curve", "Individual type II ROC curves"), col = c("royalblue3", "grey"), lwd = c(5, 1), box.lwd = 0, bg = "transparent")
box(lwd = 1)
```

[P2] Plot RT against confidence. [Non terminé]
```{r fig.width=7, fig.height=6}
P2 <- subset(all_data, all_data$Phasis == 2)
P2 <- subset(P2, P2$RT1_brut < 4000)
RT_confidence <- tapply(P2$RT1_brut, P2$Confidence, mean)

#matrix <- matrix(, nrow = 2,, ncol = length(g))
#RT_confidence <- 

cor.test(sort(unique(P2$Confidence)), RT_confidence)

plot(sort(unique(P2$Confidence)), RT_confidence, pch = 20, frame.plot = FALSE, xlab = "Confidence ratings", ylab = "Perceptual reaction time", main = "Correlation between confidence and perceptual RT during P2", xaxs = "i", yaxs = "i", ylim = c(800, 2500))
abline(lm(RT_confidence~sort(unique(P2$Confidence))), col = "red", lwd = 3)
text(80, 2000, paste("r =", as.character(round(cor.test(sort(unique(P2$Confidence)), RT_confidence)$estimate, digits = 2)), "\n", "p =", as.character(round(cor.test(sort(unique(P2$Confidence)), RT_confidence)$p.value, digits = 13))))
```

[P2] Plot performance against level of information (None, +0, +5, +10, +15) for the 3 basal performance ().
```{r fig.width=7, fig.height=6}
# Faire ça uniquement pour P2 (pour l'instant) afin de montrer que le 2e échantillon apporte de l'information

P2 <- subset(all_data, all_data$Phasis == 2)
performance_IS <- table(P2$Accuracy, P2$Inc_perf, P2$A_perf)#/nrow(P2)

plot.new()
plot.window(xlim  = c(1, 5), ylim = c(0.5, 1), xaxs = "i", yaxs = "i")
grid(4, 5, lty = 2, lwd = 0.5)

for (basal_performance in seq(1, length(unique(P2$A_perf)))) {
  for (increasing_performance in seq(1, 5)) {
    performance_IS[,increasing_performance,basal_performance] <- performance_IS[,increasing_performance,basal_performance]/sum(performance_IS[,increasing_performance,basal_performance])
  }
  
  lines(performance_IS[2,,basal_performance], col = "blue")
  lines(x = seq(1, 5), y = c(sort(unique(P2$A_perf))[basal_performance], sort(unique(P2$A_perf))[basal_performance] + 0, sort(unique(P2$A_perf))[basal_performance] + 0.05, sort(unique(P2$A_perf))[basal_performance] + 0.10, sort(unique(P2$A_perf))[basal_performance] +  0.15), col = "red")
}

axis(1, at = seq(1, 5), labels = rownames(gain_matrix))
axis(2)

# Maintenant faire ça en soustrayant le niveau basal de performance

plot.new()
plot.window(xlim  = c(1, 5), ylim = c(0, 0.25), xaxs = "i", yaxs = "i")
grid(4, 5, lty = 2, lwd = 0.5)

for (basal_performance in seq(1, 3)) {
  for (increasing_performance in seq(1, 5)) {
    performance_IS[,increasing_performance,basal_performance] <- performance_IS[,increasing_performance,basal_performance]/sum(performance_IS[,increasing_performance,basal_performance])
  }
  
  lines(performance_IS[2,,basal_performance] - sort(unique(P2$A_perf))[basal_performance], col = "blue")
  lines(x = seq(1, 5), y = c(0, 0, .05, .10, .15), col = "red")
}

axis(1, at = seq(1, 5), labels = rownames(gain_matrix))
axis(2)
```

[P2] Fit the LBA model.
#```{r fig.width=7, fig.height=6}
source("pq-lba.r")
qps = seq(0.1, 0.9, 0.2)
trim = c(180, 10000)

rawdata = subset(all_data, all_data$Phasis == 2, select = c(Condition, Accuracy, RT1_brut))
colnames(rawdata) <- c("difficulty", "correct", "rt")

use = (rawdata$rt > trim[1]) & (rawdata$rt < trim[2])
nms = list(c("err","crct"), c("1", "2", "3", "4", "5", "6", "7", "8", "9", "10", "11", "12", "13", "14", "15"))
dims = unlist(lapply(nms, length))

q = tapply(rawdata$rt[use], list(rawdata$correct[use], rawdata$difficulty[use]), quantile, probs = qps)
q = array(unlist(q), dim = c(length(qps), dims), dimnames = c(list(qps), nms))
n = tapply(rawdata$rt[use], list(rawdata$correct[use], rawdata$difficulty[use]), length)
p = tapply(rawdata$correct[use], list(rawdata$difficulty[use]), mean)

dimnames(n) = nms ; dimnames(p) = nms[-1]
pb = array(rep(n, each = length(qps) + 1), dim = c(length(qps) + 1, dim(n)), dimnames = c(list(NULL) ,dimnames(n)))*c(.1,.2,.2,.2,.2,.1)
data = list(p = p, n = n, q = q, pb = pb)
ndrifts = 15

fit = fitter(dat = data, maxit = c(100, 200, 500, 1000, 2000, 5000), nc = ndrifts)
pars = c(exp(fit[1:4]), fit[5:19])

pars[4] = pars[4] + pars[2]
pars[5:7] = 1 - pars[5:7]
fit[5:7] = 1 - fit[5:7]

names(pars) = c("s","A","ter","b","0.65","0.75","0.85","0.65+0.00","0.75+0.00","0.85+0.00","0.65+0.05","0.75+0.05","0.85+0.05","0.65+0.10","0.75+0.10","0.85+0.10","0.65+0.15","0.75+0.15","0.85+0.15")
print(pars, 3)

plot.new()
plot.window(xlim  = c(0, 2000), ylim = c(0, 1200), xaxs = "i", yaxs = "i")
grid(10, 5, lty = 2, lwd = 0.5)

for (drift_rate in seq(5, 19)) {
  abline(a = pars[2], b = pars[drift_rate], col = "green")
}

abline(h = pars[4], col = "red")

```

[P3] Create the OPIS (Optimal Proactive Information Seeking) model.
```{r [P3] OPIS model, fig.width=7, fig.height=6}
# Se restreindre aux niveaux de performance initiaux >= .70

#Créér une matrice de gain en fonction des réponses (correctes et erreurs) et du niveau d'information demandé.
if (design == AFC[1]) {
  gain_matrix <- matrix(c(130, -190, 110, -110, 90, -90, 70, -70, 50, -50), nrow = 5, ncol = 2, byrow = TRUE)
}
if (design == AFC[2]) {
  gain_matrix <- matrix(c(200, -200, 160, -110, 140, -90, 100, -70, 60, -50), nrow = 5, ncol = 2, byrow = TRUE)
}
colnames(gain_matrix) <- c("Right answer", "Wrong answer")
rownames(gain_matrix) <- c("No", "+0%", "+5%", "+10%", "+15%")
facility_levels = c(0, 0.05, 0.1, 0.15) # Without immediate answer
performance_list <- seq(chancelevel, 1 - max(facility_levels) - 0.01, (1 - max(facility_levels) - chancelevel - 0.01)/15)
print(gain_matrix)

# Trouver le nombre de point moyen gagné par trial en fonction de la probabilité d'avoir une réponse correcte (performance finale) et le nombre de points (positif et négatif) qu'engendre chaque niveau d'information supplémentaire.
model <- matrix(rep(NA, length(performance_list)*length(gain_matrix[,1])), nrow = length(performance_list), ncol = length(gain_matrix[,1]))
temp = c(0, facility_levels)
for (p in seq(1:length(performance_list))) {
  for (f in seq(1:length(gain_matrix[,1]))) {
    model[p,f] <- (gain_matrix[f,1] * (performance_list[p] + temp[f])) + (gain_matrix[f,2] * (1 - (performance_list[p] + temp[f])))
  }
}
colnames(model) <- c("No", "+0%", "+5%", "+10%", "+15%")
rownames(model) <- round(performance_list, 2)
print(model)

# Trouver, parmi cette matrice, le niveau d'information supplémentaire assurant la plus grand nombre de point (croisement entre performance finale et gain potentiel).
optim <- rep(NA, length(performance_list))
values <- rep(NA, length(performance_list))
for (i in seq(1, length(performance_list), 1)) {
  value <- max(which(model[i,] == max(model[i,]), arr.ind = TRUE))
  values[i] <- value
  optim[i] <- colnames(model)[value]
}
print(optim)

# Plotter l'évolution du nombre de points théoriquement obtenus pour chaque niveau d'information supplémentaire dans chaque situation de performance initiale.
heatcol <- rev(heat.colors(length(colnames(model))))
barplot(t(model), beside = TRUE, ylim = c(min(model)-10, max(model) + 10), col = heatcol, xlab = "Initial performance", ylab = "Mean points")
for (i in seq(1, length(performance_list), 1)) {
  text(x = (performance_list[i]*270) - 135, y = max(model) + 5, optim[i]) 
}
legend(x = length(colnames(model))*length(rownames(model)), y = -(abs(0-min(model)))/3, colnames(model), box.lwd = 0, pch = 15, col = (heatcol))

# Plotter l'évolution optimale du switch entre les niveaux d'information supplémentaire à demandé dans le but d'avoir le maximum de points possible.
seek <- rep(NA, length(values))
info = c(-0.05, facility_levels)
for (i in seq(1, length(values), 1)) {
  seek[i] = info[values[i]]
}
plot(x = performance_list, y = seek, type = "b", pch = 15, lwd = 2, col = "orange", frame.plot = FALSE, xlim = c(chancelevel, 1 - max(facility_levels)), xlab = "Initial performance", ylab = "Proactive information seeking coefficient", main = "Optimal Proactive Information Seeking model")

for (level in seq(1, length(seek))) {
  if (seek[level] == 0.15) {seek[level] = 5}
  if (seek[level] == 0.10) {seek[level] = 4}
  if (seek[level] == 0.05) {seek[level] = 3}
  if (seek[level] == 0) {seek[level] = 2}
  if (seek[level] == -0.05) {seek[level] = 1}
}
```

[P3] 
```{r delta OPIS, fig.width=7, fig.height=6}
heatcol <- rev(heat.colors(nrow(IS)))
heatcol[4] <- "darkorange2"

plot.new()
plot.window(xlim  = c(0.5, 0.84), ylim = c(1, 5), xaxs = "i", yaxs = "i")
grid(length(seek), 6, lty = 2, lwd = 0.5)

subject_IS <- matrix(, nrow = N, ncol = length(seek))
mean_IS <- rep(0, length(seek))
delta_OPIS <- rep(0, length(seek))

for (subject in subjects) {
  subject_table <- subset(all_data, all_data$Number == subject & all_data$Phasis == 3)
  IS_table <- table(subject_table$A_perf, factor(subject_table$Inc_perf, levels = c(-1, 0, 0.05, 0.10, 0.15)))
  IS_table <- IS_table/sum(IS_table[1,])
  colnames(IS_table) <- c("No", "+0%", "+5%", "+10%", "+15%")
  
  for (basal_performance in seq(1, nrow(IS_table))) {
    # Version avec le niveau moyen de recherche d'information
    subject_IS[subject, basal_performance] <- IS_table[basal_performance,1]*1 + IS_table[basal_performance,2]*2 + IS_table[basal_performance,3]*3 + IS_table[basal_performance,4]*4 + IS_table[basal_performance,5]*5
    
    # Version avec le niveau de recherche d'information le plus souvent choisit
    # subject_IS[subject, basal_performance] <- min(which(IS_table[basal_performance,] == max(IS_table[basal_performance,1], IS_table[basal_performance,2], IS_table[basal_performance,3], IS_table[basal_performance,4], IS_table[basal_performance,5])))
  }
  
  lines(x = as.numeric(rownames(IS_table)), y = subject_IS[subject,], col = "grey")
  
  mean_IS <- mean_IS + subject_IS[subject,]
  delta_OPIS[subject] <- sum(abs(subject_IS[subject,]-seek))/length(seek)
  
  #seek[1:7]<-NA
  #delta_OPIS[subject] <- sum((na.omit(subject_IS[subject,])-na.omit(seek))^2)/length(na.omit(seek))
}

lines(x = as.numeric(rownames(IS_table)), y = mean_IS/N, col = "green", lwd = 5)
lines(x = as.numeric(rownames(IS_table)), y = seek, col = "red", lwd = 5)

axis(1, at = as.numeric(rownames(IS_table)))
axis(2, at = c(1,2,3,4,5), labels = rownames(gain_matrix))

title(xlab = "Basal performance", ylab = "Information seeking level", main = "Information seeking behavior during P3")
box(lwd = 1)
```

[P3] Plot the amount of information seeking for each basal performance (and the optimal level).
```{r fig.width=10, fig.height=10}
P3 <- subset(all_data, all_data$Phasis == 3)
IS <- table(P3$Inc_perf, P3$A_perf)
for (basal_performance in seq(1, ncol(IS))) {
  IS[,basal_performance] = IS[,basal_performance]/sum(IS[,basal_performance])
}

plot.new()
plot.window(xlim  = c(0.5, 1), ylim = c(0, 1), xaxs = "i", yaxs = "i")
heatcol <- rev(heat_hcl(nrow(IS))) #heat.colors #heatcol[4] = "darkorange2"

barplot(IS, col = heatcol, space = 0, border = NA, xaxs = "i", yaxs = "i")
abline(v = 7, lty = 2)

axis(1)
axis(2, yaxp = c(0, 1, 10))
title(xlab = "Sigmoidal evoked basal performance (first sample)", ylab = "Frequency of information seeking levels", main = "Dynamics of information seeking behavior according to initial difficulty during P3")
legend("bottomright", legend = c("+15%", "+10%", "+5%", "+0%", "No"), rownames(IS), pch = 15, col = rev(heatcol), bg = "white")
box(lwd = 1)
```

[P3] Plot confidence ratings against information seeking level.
```{r [P3] Confidence x IS, fig.width=7, fig.height=6}
CIS <- table(P3$Inc_perf, P3$Confidence)#factor(P3$Confidence, levels = 0:100))

for (confidence in seq(1, length(colnames(CIS)))) {
  CIS[,confidence] <- CIS[,confidence]/sum(CIS[,confidence])
}

heatcol <- rev(heat_hcl(nrow(IS)))
barplot(CIS, col = heatcol, space = 0, border = NA)

title(xlab = "Sigmoidal evoked basal performance (first sample)", ylab = "Frequency of information seeking levels", main = "Dynamics of information seeking behavior according to initial difficulty")
legend("bottomright", legend = c("+15%", "+10%", "+5%", "+0%", "No"), rownames(IS), pch = 15, col = rev(heatcol))
```

[P3] RT1 x ISL
```{r}
plot(P3$Inc_perf, P3$RT1_brut)
oneway.test(P3$RT1_brut ~ as.character(P3$Inc_perf), P3)
tapply(P3$RT1_brut, P3$Inc_perf, mean)
barplot(tapply(P3$RT1_brut, P3$Inc_perf, mean))
a = aov(lm(P3$RT1_brut ~ as.character(P3$Inc_perf)))
TukeyHSD(a)
```


[P3] Use the LBA parameters and correlate them with the information seeking dynamic.
```{r fig.width=7, fig.height=6}
#LBA_inbut <- subset(all_data, all_data$Phasis == 3, select = c(all_data$RT1_brut, all_data$Condition, all_data$Accuracy))
```